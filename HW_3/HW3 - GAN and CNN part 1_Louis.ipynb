{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "threaded-livestock",
   "metadata": {},
   "source": [
    "\n",
    "# LINMA2472 - Algorithms in Data Science\n",
    "# Homework 3: GAN and CNN to conquer MNIST\n",
    "# Part 1: Implementation\n",
    "Bastien Massion - bastien.massion@uclouvain.be\n",
    "\n",
    "09 November 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-opinion",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "This homework is done in Python, using the PyTorch library for neural networks. \n",
    "\n",
    "The homework is divided in 2 parts, this Notebook is the assignment for part 1. The second part will become available next week. The goal of the homework is to use a GAN for generating handwritten numbers (and letters) and a CNN to classify them. This first part is dedicated to the implementation, the second part will focus on analyzing and using the neural networks.\n",
    "\n",
    "The exact instructions for the report will come with the second part. However, every task asked in this Notebook will be useful, so get ahead!\n",
    "\n",
    "As for the previous homeworks, you are making groups of 3, in the Moodle activity \"Group choice for assignment 3\".\n",
    "\n",
    "The deadline for the whole homework is: 30 November 2022, 23:59.\n",
    "\n",
    "Your questions should be posted on the dedicated Moodle \"Class forum\" or sent directly to the mail address: bastien.massion@uclouvain.be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-metabolism",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Since their introduction in 2014 by Goodfellow et al., Generative Adversarial Networks (GAN) have taken a predominant place in the deep learning landscape. GANs are deep neural networks composed of two parts: a generator and a discriminator. Those two parts are trained as opponents (hence \"adversarial\"): from the dataset, the generator tries to create new fake data that can fool the discriminator into thinking they are real, while the discriminator tries to distinguish between true and fake data. Often, the most interesting part of this model is the generator, therefore we often refer to GANs as generative models (even if they are also discriminative models). This generative property is used in a lot of applications: mainly for image processing task (style transfer, segmentation, face generation, image inpainting, deblurring, super-resolution,...), but also for natural language processing tasks (text summarization, text generation,...), for music generation or even for medical tasks (tumor detection).\n",
    "\n",
    "Another powerful architecture is Convolutional Neural Networks (CNN). CNNs naturally exploit invariances that are present in data (such that translation in images) in order, for example, to classify them. This is done by applying successive convolutions on data in order to extract the main features. Together with the backpropagation algorithm, the invention of CNN marked the revival of neural networks and deep learning in the 90's. Since then, they have found plenty of applications, mostly in image processing tasks.\n",
    "\n",
    "The goal of this homework in two parts is to implement your first GAN as well as your first CNN, to explore some properties and finally to combine them in order to generate what you desire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-combining",
   "metadata": {},
   "source": [
    "## Packages import\n",
    "You will need several packages in order to successfully build, train and use your neural networks. \n",
    "\n",
    "The following packages are necessary and should not be removed. However, if you want to add other packages for your figures, for timing or whatever, feel free to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sought-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch is a common library for neural networks in Python. torch.nn is a module for building layers of Neural Networks.\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TorchVision is part of the PyTorch environment. It is necessary to download the datasets MNIST (and EMNIST Letters)\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Usual mathematical stuff\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-entry",
   "metadata": {},
   "source": [
    "## Resources\n",
    "Training neural networks is really expensive, especially if your networks are large. It is thus important to look for the most suited computing and storing resources.\n",
    "\n",
    "In practice, parallelisation of computations can speed up the process a lot, by several orders of magnitude. In particular, computing on a GPU (Graphics Processing Unit), which has up to hundreds of cores, will be much faster than on your usual laptop CPU (Central Processing Unit), typically with less than 8 cores. \n",
    "\n",
    "Therefore, it is useful to search for the maximum available computing power.\n",
    "\n",
    "Be reassured, this homework is doable on your standard laptop CPU.\n",
    "\n",
    "But, if you want or need better equipments for this project, there are some solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-atlantic",
   "metadata": {},
   "source": [
    "### What is available?\n",
    "The function chooseDevice automatically checks if there is a GPU available. If there is one, computations will be done there. Simply be careful of adding \".to(device=device)\" everywhere it will be needed in the rest of the code. Otherwise, computations will be done on CPU.\n",
    "\n",
    "There should be no need to modify the function chooseDevice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "strong-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "\n",
    "def chooseDevice():\n",
    "    device = \"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"GPU available via cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU not available, CPU available\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "brave-manner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU available\n"
     ]
    }
   ],
   "source": [
    "device = chooseDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-disposal",
   "metadata": {},
   "source": [
    "### I need more power: Google Colab\n",
    "If you need or want more computing power, you can use Google Colab. Colab is a free and easy to use service from Google, where you can borrow their efficient resources. Often, GPU are not available (reserved in priority for people with paying subscriptions), but their CPU's are probably better than yours. You just need a Google account in order to run code online. Note that it works with Jupyter Notebooks, which is nice. \n",
    "\n",
    "If, despite this, you are not satisfied, feel free to contact Bastien Massion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-causing",
   "metadata": {},
   "source": [
    "## Set random seed\n",
    "Setting the seed for the generation of random numbers is not necessary, but can be useful for keeping consistency in your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "incident-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(2472) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-anatomy",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "Two datasets are proposed. \n",
    "\n",
    "First, the classical MNIST set. MNIST is mandatory and sufficient to complete the whole homework. It contains a total of 70 000 handwritten digits (60 000 train, 10 000 test) from 10 balanced classes (one for each digit). You have already worked with it in the HW2.\n",
    "\n",
    "Then, the EMNIST Letters set. This second dataset is larger (more data and more classes) and optional, but could be used to create more impressive results. It contains a total of 145 600 handwritten letters (124 800 train, 20 800 test) from 26 balanced classes (one for each latin letter, lowercase and uppercase mixed). For your information, EMNIST (Extended MNIST) contains several varations and extensions of MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-census",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "The datasets are loaded from the module torchvision.\n",
    "\n",
    "There should be no need to modify the function getData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "wooden-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the train and test sets from a chosen image dataset\n",
    "# Values of pixels are normalized between -1 and 1\n",
    "\n",
    "def getData(dataset = \"MNIST\", info = True):\n",
    "    if dataset == \"MNIST\":\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "        train_set = torchvision.datasets.MNIST(root=\".\", train=True, download=True, transform=transform)\n",
    "        test_set = torchvision.datasets.MNIST(root=\".\", train=False, download=True, transform=transform)\n",
    "    elif dataset == \"EMNIST_Letters\":\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "        train_set = torchvision.datasets.EMNIST(root=\".\", split=\"letters\", train=True, download=True, transform=transform)\n",
    "        test_set = torchvision.datasets.EMNIST(root=\".\", split=\"letters\", train=False, download=True, transform=transform)\n",
    "    else:\n",
    "        print(\"DATASET NOT CORRECTLY DEFINED\")\n",
    "    if info:\n",
    "        print(train_set)\n",
    "        print(test_set)\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "encouraging-ministry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: .\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5,), std=(0.5,))\n",
      "           )\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: .\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5,), std=(0.5,))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# Choose which dataset to use\n",
    "\n",
    "dataset = \"MNIST\"\n",
    "#dataset = \"EMNIST_Letters\"\n",
    "\n",
    "train_set, test_set = getData(dataset = dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-approach",
   "metadata": {},
   "source": [
    "### Get size of dataset\n",
    "This function is just there to extract useful information about the (relative) sizes of the train and test sets.\n",
    "\n",
    "There should be no need to modify the function dataSize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "southeast-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the size of the train and test sets.\n",
    "\n",
    "def dataSize(train_set, test_set):\n",
    "    n_train_set = train_set.__len__()\n",
    "    n_test_set = test_set.__len__()\n",
    "    n_tot = n_train_set + n_test_set\n",
    "    ratio_train_test = n_train_set / n_test_set\n",
    "    percentage_train = n_train_set / n_tot\n",
    "    percentage_test = 1.0 - percentage_train\n",
    "    return n_train_set, n_test_set, n_tot, ratio_train_test, percentage_train, percentage_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "settled-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_set, n_test_set, n_tot, ratio_train_test, percentage_train, percentage_test = dataSize(train_set, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-shakespeare",
   "metadata": {},
   "source": [
    "### Divide dataset into batches\n",
    "Computing the whole gradient for neural networks is too heavy, as it requires information about the whole dataset. Thus, we use (Mini-)Batch Gradient Descent. This is a variant of Stochastic Gradient Descent (SGD) in which we compute the gradient only for a small number of data, gathered in a batch. Typically, the batch size is orders of magnitude smaller than the whole dataset.\n",
    "\n",
    "The function divideInBatches splits the dataset and loads the data.\n",
    "\n",
    "You can modify the value of parameter batch_size as you wish.\n",
    "\n",
    "There should be no need to modify the function divideInBatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "still-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramater for the batch size\n",
    "# Predefined value = 32\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "juvenile-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset into batches\n",
    "\n",
    "def divideInBatches(train_set, test_set, batch_size, n_train_set, n_test_set):\n",
    "    # Train set\n",
    "    n_batches_total = math.ceil(n_train_set/batch_size)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Test set\n",
    "    n_batches_total_test = math.ceil(n_test_set/batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, test_loader, n_batches_total, n_batches_total_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "indie-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, n_batches_total, n_batches_total_test = divideInBatches(train_set, test_set, batch_size, n_train_set, n_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-stopping",
   "metadata": {},
   "source": [
    "### Correct EMNIST Letters data\n",
    "While the MNIST dataset is ready to be used, the EMNIST Letters data need to be slighty modified before use.\n",
    "\n",
    "On the one hand, the x and y axes of the images are flipped, which is unpractical for the visualization.\n",
    "\n",
    "On the other hand, the letters labels go from 1 to 26 (corresponding with their place in the alphabet) instead of from 0 to 25. This could be problematic and is better fixed directly.\n",
    "\n",
    "Therefore, every time a batch is loaded, it should go through a little correcting function first.\n",
    "\n",
    "There should be no need to modify the function correctData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "signal-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrects EMNIST Letters data\n",
    "\n",
    "def correctData(real_samples, mnist_labels, dataset):\n",
    "    if dataset == \"EMNIST_Letters\":\n",
    "        real_samples = torch.transpose(real_samples,2,3)\n",
    "        mnist_labels -= 1\n",
    "    return real_samples, mnist_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "palestinian-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_samples, mnist_labels = next(iter(train_loader))\n",
    "real_samples, mnist_labels = correctData(real_samples, mnist_labels, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-collector",
   "metadata": {},
   "source": [
    "## Show images\n",
    "During the whole homework, it will be important to show samples, either from the dataset, either ones your generate. Here is just a function that can help you visualizing and avoiding losing any plot that you once created. \n",
    "\n",
    "The function showImages allows to represent up to 16 images. You can precise if they are generated by your generator or not (i.e. coming from the dataset). You can also show the true labels for original images and the predicted labels if you use your classifier.\n",
    "\n",
    "The created figures are stored in the folder that you should create : ./Figures. The figures are stored with a predifined name depending on the (date)time. \n",
    "\n",
    "You can of course modify this function as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "saved-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot digits or letters\n",
    "\n",
    "def showImages(samples, true_labels = None, predicted_labels = None, generated = False, predicted = False, epoch = None):\n",
    "    \n",
    "    if generated == True and predicted == True:\n",
    "        plt.close(\"Generated predicted samples\")\n",
    "        f = plt.figure(\"Generated predicted samples\", figsize = (20,15))\n",
    "        f.suptitle(\"Generated predicted samples\", fontsize = 30)\n",
    "    elif generated == True and predicted == False :\n",
    "        if epoch != None:\n",
    "            plt.close(\"Generated samples epoch \" + str(epoch))\n",
    "            f = plt.figure(\"Generated samples epoch \" + str(epoch), figsize = (20,15))\n",
    "            f.suptitle(\"Generated samples epoch \" + str(epoch), fontsize = 30)\n",
    "        else:\n",
    "            plt.close(\"Generated samples\")\n",
    "            f = plt.figure(\"Generated samples\", figsize = (20,15))\n",
    "            f.suptitle(\"Generated samples\", fontsize = 30)            \n",
    "    elif predicted == True and generated == False :\n",
    "        if epoch != None:\n",
    "            plt.close(\"Predicted samples epoch \" + str(epoch))\n",
    "            f = plt.figure(\"Predicted samples epoch \" + str(epoch), figsize = (20,15))\n",
    "            f.suptitle(\"Predicted samples epoch \" + str(epoch), fontsize = 30)\n",
    "        else:\n",
    "            plt.close(\"Predicted samples\")\n",
    "            f = plt.figure(\"Predicted samples\", figsize = (20,15))\n",
    "            f.suptitle(\"Predicted samples\", fontsize = 30)            \n",
    "    else:\n",
    "        plt.close(\"MNIST samples\")\n",
    "        f = plt.figure(\"MNIST samples\", figsize = (20,15))\n",
    "        f.suptitle(\"MNIST training samples\", fontsize = 30)\n",
    "    f.subplots_adjust(wspace=0.2, hspace=0.4)\n",
    "    plt.rc('axes', titlesize=20)\n",
    "    \n",
    "    for i in range(16):\n",
    "        if i < samples.size(0):\n",
    "            fi = f.add_subplot(4, 4, i + 1)\n",
    "            fi.imshow(samples[i].reshape(28, 28), cmap=\"gray_r\")\n",
    "            plt.xticks([])\n",
    "            plt.yticks([]) \n",
    "            if generated == True and predicted == True :\n",
    "                fi.title.set_text(\"Pred: \" + str(predicted_labels[i].detach().numpy()))\n",
    "            elif generated == True and predicted == False :\n",
    "                fi.title.set_text(\"Sample \" + str(i))\n",
    "            elif predicted == True and generated == False :\n",
    "                fi.title.set_text(\"Pred: \" + str(predicted_labels[i].detach().numpy()) +  \", true: \" + str(true_labels[i].detach().numpy()))\n",
    "            else:\n",
    "                fi.title.set_text(\"Sample \" + str(i) + \": \" + str(true_labels[i].detach().numpy()))\n",
    "    \n",
    "    now = datetime.now().strftime(\"%Y_%m_%d__%H_%M\")\n",
    "    if generated == True and predicted == True :\n",
    "        f.savefig('../Figures/Sample_generated__' + now + '__epoch_' + str(epoch) + '.png')\n",
    "    elif generated == True and predicted == False :\n",
    "        f.savefig('../Figures/Sample_generated_predicted__' + now + '__epoch_' + str(epoch) + '.png')\n",
    "    elif predicted == True and generated == False :\n",
    "        f.savefig('../Figures/Sample_predicted__' + now + '__epoch_' + str(epoch) + '.png')\n",
    "    else:\n",
    "        f.savefig('../Figures/Sample_MNIST.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "modular-despite",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Figures/Sample_MNIST.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PC\\OneDrive\\Documents\\A_studies\\2022-2023\\LINMA2472 - Algorithms in Data Science\\Projet\\HW_3\\HW3 - GAN and CNN part 1 - v2.ipynb Cellule 29\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/PC/OneDrive/Documents/A_studies/2022-2023/LINMA2472%20-%20Algorithms%20in%20Data%20Science/Projet/HW_3/HW3%20-%20GAN%20and%20CNN%20part%201%20-%20v2.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m showImages(real_samples, true_labels \u001b[39m=\u001b[39;49m mnist_labels)\n",
      "\u001b[1;32mc:\\Users\\PC\\OneDrive\\Documents\\A_studies\\2022-2023\\LINMA2472 - Algorithms in Data Science\\Projet\\HW_3\\HW3 - GAN and CNN part 1 - v2.ipynb Cellule 29\u001b[0m in \u001b[0;36mshowImages\u001b[1;34m(samples, true_labels, predicted_labels, generated, predicted, epoch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/OneDrive/Documents/A_studies/2022-2023/LINMA2472%20-%20Algorithms%20in%20Data%20Science/Projet/HW_3/HW3%20-%20GAN%20and%20CNN%20part%201%20-%20v2.ipynb#X40sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     f\u001b[39m.\u001b[39msavefig(\u001b[39m'\u001b[39m\u001b[39m../Figures/Sample_predicted__\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m now \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__epoch_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(epoch) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.png\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/OneDrive/Documents/A_studies/2022-2023/LINMA2472%20-%20Algorithms%20in%20Data%20Science/Projet/HW_3/HW3%20-%20GAN%20and%20CNN%20part%201%20-%20v2.ipynb#X40sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/PC/OneDrive/Documents/A_studies/2022-2023/LINMA2472%20-%20Algorithms%20in%20Data%20Science/Projet/HW_3/HW3%20-%20GAN%20and%20CNN%20part%201%20-%20v2.ipynb#X40sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     f\u001b[39m.\u001b[39;49msavefig(\u001b[39m'\u001b[39;49m\u001b[39m../Figures/Sample_MNIST.png\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\figure.py:3274\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[1;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[0;32m   3270\u001b[0m     \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes:\n\u001b[0;32m   3271\u001b[0m         stack\u001b[39m.\u001b[39menter_context(\n\u001b[0;32m   3272\u001b[0m             ax\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39m_cm_set(facecolor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m, edgecolor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m-> 3274\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcanvas\u001b[39m.\u001b[39mprint_figure(fname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\backend_bases.py:2338\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2335\u001b[0m     \u001b[39m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2336\u001b[0m     \u001b[39m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2337\u001b[0m     \u001b[39mwith\u001b[39;00m cbook\u001b[39m.\u001b[39m_setattr_cm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, dpi\u001b[39m=\u001b[39mdpi):\n\u001b[1;32m-> 2338\u001b[0m         result \u001b[39m=\u001b[39m print_method(\n\u001b[0;32m   2339\u001b[0m             filename,\n\u001b[0;32m   2340\u001b[0m             facecolor\u001b[39m=\u001b[39mfacecolor,\n\u001b[0;32m   2341\u001b[0m             edgecolor\u001b[39m=\u001b[39medgecolor,\n\u001b[0;32m   2342\u001b[0m             orientation\u001b[39m=\u001b[39morientation,\n\u001b[0;32m   2343\u001b[0m             bbox_inches_restore\u001b[39m=\u001b[39m_bbox_inches_restore,\n\u001b[0;32m   2344\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2345\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   2346\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\backend_bases.py:2204\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2200\u001b[0m     optional_kws \u001b[39m=\u001b[39m {  \u001b[39m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2201\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfacecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39medgecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39morientation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2202\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbbox_inches_restore\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m   2203\u001b[0m     skip \u001b[39m=\u001b[39m optional_kws \u001b[39m-\u001b[39m {\u001b[39m*\u001b[39minspect\u001b[39m.\u001b[39msignature(meth)\u001b[39m.\u001b[39mparameters}\n\u001b[1;32m-> 2204\u001b[0m     print_method \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mwraps(meth)(\u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: meth(\n\u001b[0;32m   2205\u001b[0m         \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m skip}))\n\u001b[0;32m   2206\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     print_method \u001b[39m=\u001b[39m meth\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:410\u001b[0m, in \u001b[0;36mdelete_parameter.<locals>.wrapper\u001b[1;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m     deprecation_addendum \u001b[39m=\u001b[39m (\n\u001b[0;32m    401\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIf any parameter follows \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m!r}\u001b[39;00m\u001b[39m, they should be passed as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mkeyword, not positionally.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    403\u001b[0m     warn_deprecated(\n\u001b[0;32m    404\u001b[0m         since,\n\u001b[0;32m    405\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39mrepr\u001b[39m(name),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    408\u001b[0m                  \u001b[39melse\u001b[39;00m deprecation_addendum,\n\u001b[0;32m    409\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 410\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39minner_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minner_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:517\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[39m@_api\u001b[39m\u001b[39m.\u001b[39mdelete_parameter(\u001b[39m\"\u001b[39m\u001b[39m3.5\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_png\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, \u001b[39m*\u001b[39margs,\n\u001b[0;32m    470\u001b[0m               metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, pil_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    471\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_print_pil(filename_or_obj, \u001b[39m\"\u001b[39;49m\u001b[39mpng\u001b[39;49m\u001b[39m\"\u001b[39;49m, pil_kwargs, metadata)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:464\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[39mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[39m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    463\u001b[0m FigureCanvasAgg\u001b[39m.\u001b[39mdraw(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 464\u001b[0m mpl\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mimsave(\n\u001b[0;32m    465\u001b[0m     filename_or_obj, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer_rgba(), \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49mfmt, origin\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mupper\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    466\u001b[0m     dpi\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdpi, metadata\u001b[39m=\u001b[39;49mmetadata, pil_kwargs\u001b[39m=\u001b[39;49mpil_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\image.py:1664\u001b[0m, in \u001b[0;36mimsave\u001b[1;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m   1662\u001b[0m pil_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mformat\u001b[39m)\n\u001b[0;32m   1663\u001b[0m pil_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, (dpi, dpi))\n\u001b[1;32m-> 1664\u001b[0m image\u001b[39m.\u001b[39msave(fname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:2350\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2348\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mr+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2349\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2350\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mw+b\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   2352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2353\u001b[0m     save_handler(\u001b[39mself\u001b[39m, fp, filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Figures/Sample_MNIST.png'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x1080 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "showImages(real_samples, true_labels = mnist_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-donna",
   "metadata": {},
   "source": [
    "## Section 1: Train a GAN on MNIST for image generation\n",
    "In this first part, you are asked to train and test a GAN for the generation of fake handwritten numbers (and letters). \n",
    "\n",
    "Let's just recall what training a model means in machine learning. It is in fact quite simple: given a model architecture, use an optimization scheme to minimize a loss function (representing the gap between the data and the model predictions) in order to find the optimal values of the model parameters. In this Section, all of these elements are treated one by one.\n",
    "\n",
    "Moreover, you receive reference Generator and Discriminator architectures. These neural network architectures should not be changed. Indeed, they are sufficient in order to get nice enough results while keeping the training time reasonable on a laptop CPU (normally less than 2 hours training).\n",
    "\n",
    "However, if you want to train additional Generator and/or Discriminator models that could be larger or smaller (for improved results or faster training for example), feel free to create them by yourself, explain them with comments and name them differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-eagle",
   "metadata": {},
   "source": [
    "### 1.a Generator class\n",
    "The Generator class takes an input: the dimension of the latent space. Indeed, the generator will create an image (1x28x28) from an input latent vector (1 color i.e. gray-scale, 28x28 pixels). Typically, this vector is of dimension 100 with each entry sampled from a standard normal distribution or from an uniform distribution on [-1,1]. You are free to change this dimension and see what happens.\n",
    "\n",
    "The latent space is thus a space where the information contained in the images is compressed/condensed. This latent space will be the focus of the second part of the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "patent-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of the latent space\n",
    "# Predefined value = 100\n",
    "\n",
    "dim_latent_space = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-crack",
   "metadata": {},
   "source": [
    "The class below contains the reference Generator architecture. You don't have to worry about the initialization of the model, it is done automatically.\n",
    "\n",
    "This architecture should not be changed.\n",
    "\n",
    "The Generator takes a batch of latent vectors as input, and ouputs one 1x28x28 pixels image for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "outer-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_latent_space):\n",
    "        super().__init__()\n",
    "        self.dim_latent_space = dim_latent_space\n",
    "        self.fc = nn.Linear(self.dim_latent_space, 64*7*7)\n",
    "        self.trans_conv1 = nn.ConvTranspose2d(64, 64, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
    "        self.trans_conv2 = nn.ConvTranspose2d(64, 32, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(32)\n",
    "        self.trans_conv3 = nn.ConvTranspose2d(32, 16, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(16)\n",
    "        self.trans_conv4 = nn.ConvTranspose2d(16, 1, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "        \n",
    "    def forward(self, x):                   # Input = batch_size*dim_latent_space\n",
    "        x = self.fc(x)                      # Output = batch_size*(64*7*7)\n",
    "        x = x.view(-1, 64, 7, 7)            # Output = batch_size*64*7*7\n",
    "        x = self.trans_conv1(x)             # Output = batch_size*64*14*14\n",
    "        x = F.relu(self.batch_norm1(x))\n",
    "        x = self.trans_conv2(x)             # Output = batch_size*32*14*14\n",
    "        x = F.relu(self.batch_norm2(x))\n",
    "        x = self.trans_conv3(x)             # Output = batch_size*16*14*14\n",
    "        x = F.relu(self.batch_norm3(x))     \n",
    "        x = self.trans_conv4(x)             # Output = batch_size*1*28*28\n",
    "        x = torch.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "numerical-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(dim_latent_space).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-resource",
   "metadata": {},
   "source": [
    "### 1.b Discriminator class\n",
    "The class below contains the reference Discriminator architecture. You don't have to worry about the initialization of the model, it is done automatically.\n",
    "\n",
    "This architecture should not be changed.\n",
    "\n",
    "The Discriminitor takes a batch of 1x28x28 pixels images as input, and ouputs numbers between -inf and +inf. The activation of the last layer represents some likelihood of the image being fake or not, according to the model. Afterwards, it goes through a sigmoid function in order to scale numbers between 0 and 1. An output of 0 means that the model is certain that the corresponding input image is fake, and an output of 0 means that it is certain that it is real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "julian-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(1, 32, kernel_size = 3, stride = 2, padding = 1)     \n",
    "        self.conv0_drop = nn.Dropout2d(0.25)\n",
    "        self.conv1 = nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.conv1_drop = nn.Dropout2d(0.25)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.conv2_drop = nn.Dropout2d(0.25)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv3_drop = nn.Dropout2d(0.25)\n",
    "        self.fc = nn.Linear(128*7*7, 1)\n",
    "    \n",
    "    def forward(self, x):                               # Input = batch_size*1*28*28\n",
    "        x = x.view(-1, 1, 28, 28)                       # Output = batch_size*1*28*28\n",
    "        x = F.leaky_relu(self.conv0(x), 0.2)            # Output = batch_size*32*14*14\n",
    "        x = self.conv0_drop(x)\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)            # Output = batch_size*64*14*14\n",
    "        x = self.conv1_drop(x)\n",
    "        x = F.leaky_relu(self.conv2(x), 0.2)            # Output = batch_size*128*14*14\n",
    "        x = self.conv2_drop(x)\n",
    "        x = F.leaky_relu(self.conv3(x), 0.2)            # Output = batch_size*128*7*7\n",
    "        x = self.conv3_drop(x)\n",
    "        x = x.view(-1, 128*7*7)                         # Output = batch_size*(128*7*7)\n",
    "        x = self.fc(x)                                  # Output = batch_size*1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "realistic-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator().to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-checkout",
   "metadata": {},
   "source": [
    "### 1.c Number of parameters\n",
    "This function counts and prints the number of parameters in any neural network model.\n",
    "\n",
    "There should be no need to modify the function numberParameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "communist-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberParameters(model, trainable = False, model_name = None):\n",
    "    total_params = sum(param.numel() for param in model.parameters())\n",
    "    if model_name != None:\n",
    "        print(\"Number of parameters of \" + model_name + \": \" + str(total_params))\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "irish-community",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of discriminator: 246529\n",
      "Number of parameters of generator: 377121\n"
     ]
    }
   ],
   "source": [
    "discriminator_params = numberParameters(discriminator, model_name = \"discriminator\")\n",
    "generator_params = numberParameters(generator, model_name = \"generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-copying",
   "metadata": {},
   "source": [
    "### 1.d Loss function\n",
    "The loss function measures how good the model is. For the discriminator, it a binary classification problem: the discriminator tries to distinguish true data (label = 1) from fake generated ones (label = 0). Therefore, we use a Binary Cross Entropy loss. As the only goal of the generator is to fool the discriminator, it can also be expressed with a Binary Cross Entropy loss function. Think about what inputs we need to give to the loss function for the discriminator and for the generator.\n",
    "\n",
    "You see that BCELoss is replaced by BCEWithLogitsLoss. Think about why this is, and how it links with the last layer of the given Discriminator architecture.\n",
    "\n",
    "This loss function should not be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "friendly-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for the GAN\n",
    "\n",
    "#loss_function_gan = nn.BCELoss()\n",
    "loss_function_gan = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-exclusive",
   "metadata": {},
   "source": [
    "### 1.e Optimizers\n",
    "Adam is one of the most famous adaptive stochastic gradient descent algorithm. It is massively used for training neural networks.\n",
    "\n",
    "You can play around with the Adam parameters if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "identical-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimization parameters\n",
    "# Predefined values: lr_gan = 0.0002, betas_gan = (0.5, 0.999)\n",
    "lr_gan = 0.0002\n",
    "betas_gan = (0.5, 0.999)\n",
    "\n",
    "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr_gan, betas = betas_gan)\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr_gan, betas = betas_gan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-crowd",
   "metadata": {},
   "source": [
    "### 1.f Accuracy\n",
    "We define one error measure as the accuracy reached by the discriminator for its predictions y_predicted, i.e. the ratio between the number of correct classifications and the total number of samples. We suppose that if 0.0 <= y_generated[i] <= 0.5, then it predicts a fake image and if 0.5 < y_generated[i] <= 1.0, it predicts a true image.\n",
    "\n",
    "During the training, data are loaded batch by batch. It could thus be a good idea to consider that the inputs of the following function are the true and predicted labels from one batch of data.\n",
    "\n",
    "Task: complete the computeGANAccuracy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "blessed-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGANAccuracy(y_predicted,y_true):\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    \n",
    "    # TO COMPLETE\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-shanghai",
   "metadata": {},
   "source": [
    "### 1.g Preparing the training process\n",
    "In this part, you are asked to initialize the training process (not the model which is initialized automatically). \n",
    "\n",
    "Let's just first define the number of iterations (epochs) that the training will last. Satisfactory results can be obtained with 5 epochs, but you can of course increase this value for a better training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "continued-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs for the training\n",
    "# Predefined value: num_epochs_gan = 5\n",
    "\n",
    "num_epochs_gan = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-tomato",
   "metadata": {},
   "source": [
    "It is important to keep track of the evolution of the error metrics during the training process. A simple idea is to store the values of the metrics in arrays, at least after every epoch.\n",
    "\n",
    "Task: Create arrays that will contain the accuracy for train and test sets, as well as the values of the discriminator loss and of the generator loss during the training. Compute the errors before training the model and store them in your arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "announced-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error metrics at initialisation\n",
    "\n",
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-shaft",
   "metadata": {},
   "source": [
    "It is a good option to keep track of the evolution of the images generated through the training. Defining a reference sample of latent vectors allows to generate the image for the same inputs every time. You can compute the output from those vectors after each epoch to see how the generated images evolve with the training. The latent vectors can be generated from a standard normal distribution, or from a uniform distribution over [-1,1].\n",
    "\n",
    "Task: Define a sample of reference latent vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "regional-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference latent vectors\n",
    "\n",
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-estonia",
   "metadata": {},
   "source": [
    "### 1.h Training and testing the GAN\n",
    "This is finally the core part of this section: training your GAN.\n",
    "\n",
    "The training will follow an easy structure. Here is the pseudo-code of the training process:\n",
    "\n",
    "\n",
    "    for each epoch:\n",
    "\n",
    "        for each batch:\n",
    "\n",
    "            train the discriminator on the batch:\n",
    "\n",
    "                generate fake data (same amount as size of the batch)\n",
    "\n",
    "                predict if data from the batch and generated data are fake or not\n",
    "\n",
    "                compute the discriminator loss function\n",
    "\n",
    "                backpropagate the error in the discriminator\n",
    "\n",
    "            train the generator on the batch\n",
    "\n",
    "                generate fake data (same amount as size of the batch)\n",
    "\n",
    "                predict if generated data are fake or not\n",
    "\n",
    "                compute the generator loss function\n",
    "\n",
    "                backpropagate the error in the generator\n",
    "\n",
    "        end\n",
    "\n",
    "    end\n",
    "\n",
    "Task: Code the GAN training from the pseudo-code given above. Don't forget to switch between training and evaluation modes for the models, by using train() and eval() Pytorch functions. To understand what to do and why, look for their impact on specific layers of the networks. You may also use the time package in order to get information about the duration of the training. Finally, at every epoch, compute and store the accuracy and the losses of the model and generate images from your reference sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "unusual-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of the GAN\n",
    "\n",
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-drill",
   "metadata": {},
   "source": [
    "### 1.i Plot the results\n",
    "You should plot the evolution of your error metrics (accuracy and loss function) in function of the number of epochs, to see how efficient the training is. Show the evolution of the images generated from your reference sample as well.\n",
    "\n",
    "Task: Make those plots, as clean and readable as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "veterinary-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of accuracy and of loss function, images generated from your reference sample.\n",
    "\n",
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-relations",
   "metadata": {},
   "source": [
    "### 1.j Save your models\n",
    "A really useful feature for your experiments is to be able to save your models. This can be done with the torch.save function. In the function saveModel, a model is stored in the folder ./Models that you may want to create. The (date)time is used to always store your models with a different name and never lose some training that you would have done.\n",
    "\n",
    "Be careful however: the saveModel function below stores the \"state dictionary\" of your model, not your model itself. \n",
    "\n",
    "You can of course modify the function saveModel as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "formal-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model, model_name):\n",
    "    now = datetime.now().strftime(\"%Y_%m_%d__%H_%M\")\n",
    "    torch.save(model.state_dict(), '../Models/' + model_name + '__' + now)\n",
    "\n",
    "#### Example\n",
    "#\n",
    "# D = Discriminator().to(device=device)\n",
    "# saveModel(D, \"Discriminator\")\n",
    "#\n",
    "##\n",
    "# The model D has been saved as \"Discriminator__2022_11_12__15_21\" in the folder ./Models as your code finished running at\n",
    "# 15:21 on the 12 November 2022.\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-memorabilia",
   "metadata": {},
   "source": [
    "### 1.k Load your models\n",
    "You can also load models that you trained and saved earlier with the function loadModel.\n",
    "\n",
    "However, be careful. You will have to first recreate a new instance of your class, and then import the state dictionary in the new instance. This means that the characteristics of your class should not have changed since then. In particular, the number of layers and the size of the layers need to match between the trained model and the most recent class in order to correctly assign the stored parameters. Moreover, there could be some trouble if you changed functions/methods from the class. \n",
    "\n",
    "You can of course modify the function loadModel as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "indirect-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(model, model_name, date_time):\n",
    "    model.load_state_dict(torch.load('../Models/' + model_name + '__' + date_time))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "#### Example\n",
    "#\n",
    "# D = Discriminator().to(device=device)\n",
    "# loadModel(D, \"Discriminator\", \"2022_11_12__15_21\")\n",
    "#\n",
    "##\n",
    "# The model \"Discriminator__2022_11_12__15_21\" from the folder ./Models has been loaded in the Discriminator D\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-still",
   "metadata": {},
   "source": [
    "## Section 2: Train a CNN on MNIST for image classification\n",
    "In this second part, you are asked to train and test a CNN for the classification of handwritten numbers (and letters).\n",
    "\n",
    "Therefore, you receive a reference CNN architecture that should not be changed. Training your CNN should be faster than training your GAN, so there is no fear about that.\n",
    "\n",
    "However, if you want to train additional CNN models (for improved results or faster training for example), feel free to create them by yourself and name the classes differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-boston",
   "metadata": {},
   "source": [
    "### 2.a Number of classes\n",
    "For classification task, it is of course necessary to know the number of classes possible. \n",
    "\n",
    "For the rest of the homework, pay attention not to confuse the label class (ex: the image represents a 2) and a model class (ex: the model is an instance of the CNN class).\n",
    "\n",
    "There should be no need to modify the function numberClasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "institutional-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberClasses(dataset):\n",
    "    n_classes = len(dataset.classes)\n",
    "    return n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "systematic-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = numberClasses(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-cursor",
   "metadata": {},
   "source": [
    "### 2.b CNN class\n",
    "The class below contains the reference CNN architecture. You don't have to worry about the initialization of the model, it is done automatically.\n",
    "\n",
    "This architecture should not be changed.\n",
    "\n",
    "The CNN architecture is defined by the number of classes. Indeed, for each entry it outputs a vector of size n_classes with values between -inf and +inf. This vector can be passed through the function scaleToProbabilities in order to get scaled and become somewhat interpretable as probabilities of belonging to one class. The function predictLabels extends the scaleToProbabilities function and selects the most activated/highest valued class as the CNN prediction.\n",
    "\n",
    "There should be no need to modify the functions scaleToProbabilities and predictLabels, except playing with the scale_factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "quantitative-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.conv1 = nn.Sequential(             # Images input is batch_size*1*28*28\n",
    "            nn.Conv2d(1, 16, 5, 1, 2),          # Output is batch_size*16*28*28\n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),        # Output is batch_size*16*14*14\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, 5, 1, 2),         # Output is batch_size*32*14*14\n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                    # Output is batch_size*32*7*7\n",
    "        )\n",
    "        # fully connected layer, output one number for each class\n",
    "        self.out = nn.Sequential(         \n",
    "            nn.Linear(32 * 7 * 7, self.n_classes),          # Output is batch_size*n_classes\n",
    "        )\n",
    "    \n",
    "    def scaleToProbabilities(self, outmap, scale_factor = 3): \n",
    "        # scale_factor allows to scale your output before passing it into the softmax function \n",
    "        # in order to get numbers interpratble as probabilities\n",
    "        \n",
    "        flattened_outmap = outmap.view(outmap.shape[0], -1)\n",
    "        outmap_std = torch.std(flattened_outmap, dim = 1).view(-1, 1)\n",
    "        outmap_scaled_std = torch.div(outmap, outmap_std)\n",
    "        probabilities = nn.functional.softmax(outmap_scaled_std*scale_factor, dim=1)\n",
    "        return probabilities\n",
    "    \n",
    "    def predictLabels(self, outmap, scale_factor = 3):\n",
    "        probabilities = self.scaleToProbabilities(outmap, scale_factor = 3)\n",
    "        certainty, predicted_labels = torch.max(probabilities, 1)\n",
    "        predicted_labels = predicted_labels.data.squeeze()\n",
    "        certainty = certainty.data.squeeze()\n",
    "        return predicted_labels, probabilities, certainty\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), 32 * 7 * 7)\n",
    "        output = self.out(x)\n",
    "        return output, x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "uniform-importance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of classifier: 28938\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN(n_classes).to(device=device)\n",
    "cnn_params = numberParameters(cnn, model_name = \"classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-observation",
   "metadata": {},
   "source": [
    "### 2.c Loss function\n",
    "For classification tasks, the main loss function used is Cross Entropy. As entries, it takes the activations of last layer nodes  and the true labels of data (between 0 and n_classes-1). \n",
    "\n",
    "This loss function should not be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "perceived-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for the CNN\n",
    "\n",
    "loss_function_cnn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-liver",
   "metadata": {},
   "source": [
    "### 2.d Optimizer\n",
    "Again, we will use Adam as an optimizer.\n",
    "\n",
    "You can play around with the Adam parameters if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "sharp-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimization parameters\n",
    "# Predefined values: lr_cnn = 0.01, betas_cnn = (0.9, 0.999)\n",
    "lr_cnn = 0.01\n",
    "betas_cnn = (0.9, 0.999)\n",
    "\n",
    "optimizer_cnn = torch.optim.Adam(cnn.parameters(), lr=lr_cnn, betas = betas_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-maintenance",
   "metadata": {},
   "source": [
    "### 2.e Accuracy\n",
    "We define one error measure as the accuracy reached by the CNN for its classifying predictions. In particular, we call accuracy the ratio between the number of correct classifications and the number of samples. We assume that the most activated output corresponds to the class prediction.\n",
    "\n",
    "During the training, data are loaded batch by batch. It could thus be a good idea to consider that the inputs of the following function are the true and predicted labels from one batch of data.\n",
    "\n",
    "Task: complete the computeCNNAccuracy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "residential-complement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.39"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def find_accuracy(model, data):\n",
    "    # Test the model\n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in data:\n",
    "            test_output, last_layer = cnn(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            correct += (pred_y == labels).sum().item()\n",
    "            total += test_output.size(0)\n",
    "            \n",
    "            pass\n",
    "    accuracy = (correct / total)*100\n",
    "    \n",
    "    return round(accuracy, 4)\n",
    "\n",
    "\n",
    "chat= find_accuracy(cnn, test_loader)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-slovak",
   "metadata": {},
   "source": [
    "### 2.f Preparing the training process\n",
    "In this part, you are asked to initialize the training process (not the model which is initialized automatically). \n",
    "\n",
    "Let's just first define the number of iterations (epochs) that the training will last. Satisfactory results can be obtained with 15 epochs, but you can of course increase this value for a better training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "infectious-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs for the training\n",
    "# Predefined value: num_epochs_cnn = 15\n",
    "\n",
    "num_epochs_cnn = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-designation",
   "metadata": {},
   "source": [
    "It is important to keep track of the evolution of the error metrics during the training process. A simple idea is to store the values of the metrics in arrays, at least after every epoch.\n",
    "\n",
    "Task: Create arrays that will contain the accuracy for train and test sets, as well as the values of the CNN loss during the training. Compute the errors before training the model and store them in your arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "sublime-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error metrics at initialisation\n",
    "\n",
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-england",
   "metadata": {},
   "source": [
    "### 2.g Training and testing the CNN\n",
    "This is finally the core part of this section: training your CNN.\n",
    "\n",
    "The training will follow an easy structure. Here is the pseudo-code of the training process:\n",
    "\n",
    "    for each epoch:\n",
    "\n",
    "        for each batch:\n",
    "\n",
    "            train the classifier on the batch:\n",
    "\n",
    "                predict the labels\n",
    "\n",
    "                compute the classifier loss function\n",
    "\n",
    "                backpropagate the error in the cnn\n",
    "                \n",
    "        end\n",
    "\n",
    "    end\n",
    "\n",
    "Task: Code the CNN training from the pseudo-code given above. Don't forget to switch between training and evaluation modes for the model, by using train() and eval() Pytorch functions. You may also use the time package in order to get information about the duration of the training. Finally, at every epoch, compute and store the accuracy and the loss of the model and generate images from your reference sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d41b3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5c86ae9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [3268/60000 (5.45%)], Loss: 0.0933\n",
      "Epoch [1/15], Step [6468/60000 (10.78%)], Loss: 0.4693\n",
      "Epoch [1/15], Step [9668/60000 (16.11%)], Loss: 0.0815\n",
      "Epoch [1/15], Step [12868/60000 (21.45%)], Loss: 0.0899\n",
      "Epoch [1/15], Step [16068/60000 (26.78%)], Loss: 0.0814\n",
      "Epoch [1/15], Step [19268/60000 (32.11%)], Loss: 0.0300\n",
      "Epoch [1/15], Step [22468/60000 (37.45%)], Loss: 0.0611\n",
      "Epoch [1/15], Step [25668/60000 (42.78%)], Loss: 0.1150\n",
      "Epoch [1/15], Step [28868/60000 (48.11%)], Loss: 0.0780\n",
      "Epoch [1/15], Step [32068/60000 (53.45%)], Loss: 0.0645\n",
      "Epoch [1/15], Step [35268/60000 (58.78%)], Loss: 0.5559\n",
      "Epoch [1/15], Step [38468/60000 (64.11%)], Loss: 0.1603\n",
      "Epoch [1/15], Step [41668/60000 (69.45%)], Loss: 0.0162\n",
      "Epoch [1/15], Step [44868/60000 (74.78%)], Loss: 0.0445\n",
      "Epoch [1/15], Step [48068/60000 (80.11%)], Loss: 0.1927\n",
      "Epoch [1/15], Step [51268/60000 (85.45%)], Loss: 0.3251\n",
      "Epoch [1/15], Step [54468/60000 (90.78%)], Loss: 0.1559\n",
      "Epoch [1/15], Step [57668/60000 (96.11%)], Loss: 0.1577\n",
      "Epoch: 1 Accuracy : 96.77 %\n",
      "Epoch [2/15], Step [3268/60000 (5.45%)], Loss: 0.0152\n",
      "Epoch [2/15], Step [6468/60000 (10.78%)], Loss: 0.0098\n",
      "Epoch [2/15], Step [9668/60000 (16.11%)], Loss: 0.2971\n",
      "Epoch [2/15], Step [12868/60000 (21.45%)], Loss: 0.0839\n",
      "Epoch [2/15], Step [16068/60000 (26.78%)], Loss: 0.1966\n",
      "Epoch [2/15], Step [19268/60000 (32.11%)], Loss: 0.2404\n",
      "Epoch [2/15], Step [22468/60000 (37.45%)], Loss: 0.0589\n",
      "Epoch [2/15], Step [25668/60000 (42.78%)], Loss: 0.0069\n",
      "Epoch [2/15], Step [28868/60000 (48.11%)], Loss: 0.0039\n",
      "Epoch [2/15], Step [32068/60000 (53.45%)], Loss: 0.3171\n",
      "Epoch [2/15], Step [35268/60000 (58.78%)], Loss: 0.0717\n",
      "Epoch [2/15], Step [38468/60000 (64.11%)], Loss: 0.0025\n",
      "Epoch [2/15], Step [41668/60000 (69.45%)], Loss: 0.0199\n",
      "Epoch [2/15], Step [44868/60000 (74.78%)], Loss: 0.0035\n",
      "Epoch [2/15], Step [48068/60000 (80.11%)], Loss: 0.0088\n",
      "Epoch [2/15], Step [51268/60000 (85.45%)], Loss: 0.0665\n",
      "Epoch [2/15], Step [54468/60000 (90.78%)], Loss: 0.2136\n",
      "Epoch [2/15], Step [57668/60000 (96.11%)], Loss: 0.0278\n",
      "Epoch: 2 Accuracy : 97.31 %\n",
      "Epoch [3/15], Step [3268/60000 (5.45%)], Loss: 0.5698\n",
      "Epoch [3/15], Step [6468/60000 (10.78%)], Loss: 0.0088\n",
      "Epoch [3/15], Step [9668/60000 (16.11%)], Loss: 0.0013\n",
      "Epoch [3/15], Step [12868/60000 (21.45%)], Loss: 0.1418\n",
      "Epoch [3/15], Step [16068/60000 (26.78%)], Loss: 0.0374\n",
      "Epoch [3/15], Step [19268/60000 (32.11%)], Loss: 0.1784\n",
      "Epoch [3/15], Step [22468/60000 (37.45%)], Loss: 0.0224\n",
      "Epoch [3/15], Step [25668/60000 (42.78%)], Loss: 0.0390\n",
      "Epoch [3/15], Step [28868/60000 (48.11%)], Loss: 0.0360\n",
      "Epoch [3/15], Step [32068/60000 (53.45%)], Loss: 0.3837\n",
      "Epoch [3/15], Step [35268/60000 (58.78%)], Loss: 0.1095\n",
      "Epoch [3/15], Step [38468/60000 (64.11%)], Loss: 0.0288\n",
      "Epoch [3/15], Step [41668/60000 (69.45%)], Loss: 0.1705\n",
      "Epoch [3/15], Step [44868/60000 (74.78%)], Loss: 0.3698\n",
      "Epoch [3/15], Step [48068/60000 (80.11%)], Loss: 0.0469\n",
      "Epoch [3/15], Step [51268/60000 (85.45%)], Loss: 0.3295\n",
      "Epoch [3/15], Step [54468/60000 (90.78%)], Loss: 0.0107\n",
      "Epoch [3/15], Step [57668/60000 (96.11%)], Loss: 0.1572\n",
      "Epoch: 3 Accuracy : 97.78 %\n",
      "Epoch [4/15], Step [3268/60000 (5.45%)], Loss: 0.1002\n",
      "Epoch [4/15], Step [6468/60000 (10.78%)], Loss: 0.1847\n",
      "Epoch [4/15], Step [9668/60000 (16.11%)], Loss: 0.0192\n",
      "Epoch [4/15], Step [12868/60000 (21.45%)], Loss: 0.6492\n",
      "Epoch [4/15], Step [16068/60000 (26.78%)], Loss: 0.2662\n",
      "Epoch [4/15], Step [19268/60000 (32.11%)], Loss: 0.1542\n",
      "Epoch [4/15], Step [22468/60000 (37.45%)], Loss: 0.0415\n",
      "Epoch [4/15], Step [25668/60000 (42.78%)], Loss: 0.0001\n",
      "Epoch [4/15], Step [28868/60000 (48.11%)], Loss: 0.0301\n",
      "Epoch [4/15], Step [32068/60000 (53.45%)], Loss: 0.1221\n",
      "Epoch [4/15], Step [35268/60000 (58.78%)], Loss: 0.0552\n",
      "Epoch [4/15], Step [38468/60000 (64.11%)], Loss: 0.0095\n",
      "Epoch [4/15], Step [41668/60000 (69.45%)], Loss: 0.1195\n",
      "Epoch [4/15], Step [44868/60000 (74.78%)], Loss: 0.1072\n",
      "Epoch [4/15], Step [48068/60000 (80.11%)], Loss: 0.1626\n",
      "Epoch [4/15], Step [51268/60000 (85.45%)], Loss: 0.0006\n",
      "Epoch [4/15], Step [54468/60000 (90.78%)], Loss: 0.0191\n",
      "Epoch [4/15], Step [57668/60000 (96.11%)], Loss: 0.0993\n",
      "Epoch: 4 Accuracy : 97.23 %\n",
      "Epoch [5/15], Step [3268/60000 (5.45%)], Loss: 0.0135\n",
      "Epoch [5/15], Step [6468/60000 (10.78%)], Loss: 0.3007\n",
      "Epoch [5/15], Step [9668/60000 (16.11%)], Loss: 0.1710\n",
      "Epoch [5/15], Step [12868/60000 (21.45%)], Loss: 0.4524\n",
      "Epoch [5/15], Step [16068/60000 (26.78%)], Loss: 0.0281\n",
      "Epoch [5/15], Step [19268/60000 (32.11%)], Loss: 0.0131\n",
      "Epoch [5/15], Step [22468/60000 (37.45%)], Loss: 0.0759\n",
      "Epoch [5/15], Step [25668/60000 (42.78%)], Loss: 0.1026\n",
      "Epoch [5/15], Step [28868/60000 (48.11%)], Loss: 0.2146\n",
      "Epoch [5/15], Step [32068/60000 (53.45%)], Loss: 0.2115\n",
      "Epoch [5/15], Step [35268/60000 (58.78%)], Loss: 0.2456\n",
      "Epoch [5/15], Step [38468/60000 (64.11%)], Loss: 0.1980\n",
      "Epoch [5/15], Step [41668/60000 (69.45%)], Loss: 0.0008\n",
      "Epoch [5/15], Step [44868/60000 (74.78%)], Loss: 0.0111\n",
      "Epoch [5/15], Step [48068/60000 (80.11%)], Loss: 0.0060\n",
      "Epoch [5/15], Step [51268/60000 (85.45%)], Loss: 0.1569\n",
      "Epoch [5/15], Step [54468/60000 (90.78%)], Loss: 0.0337\n",
      "Epoch [5/15], Step [57668/60000 (96.11%)], Loss: 0.0097\n",
      "Epoch: 5 Accuracy : 97.44 %\n",
      "Epoch [6/15], Step [3268/60000 (5.45%)], Loss: 0.0195\n",
      "Epoch [6/15], Step [6468/60000 (10.78%)], Loss: 0.5964\n",
      "Epoch [6/15], Step [9668/60000 (16.11%)], Loss: 0.2202\n",
      "Epoch [6/15], Step [12868/60000 (21.45%)], Loss: 0.0076\n",
      "Epoch [6/15], Step [16068/60000 (26.78%)], Loss: 0.0221\n",
      "Epoch [6/15], Step [19268/60000 (32.11%)], Loss: 0.1791\n",
      "Epoch [6/15], Step [22468/60000 (37.45%)], Loss: 0.0175\n",
      "Epoch [6/15], Step [25668/60000 (42.78%)], Loss: 0.1352\n",
      "Epoch [6/15], Step [28868/60000 (48.11%)], Loss: 0.2132\n",
      "Epoch [6/15], Step [32068/60000 (53.45%)], Loss: 0.0093\n",
      "Epoch [6/15], Step [35268/60000 (58.78%)], Loss: 0.0068\n",
      "Epoch [6/15], Step [38468/60000 (64.11%)], Loss: 0.2589\n",
      "Epoch [6/15], Step [41668/60000 (69.45%)], Loss: 0.0865\n",
      "Epoch [6/15], Step [44868/60000 (74.78%)], Loss: 0.0124\n",
      "Epoch [6/15], Step [48068/60000 (80.11%)], Loss: 0.0058\n",
      "Epoch [6/15], Step [51268/60000 (85.45%)], Loss: 0.0305\n",
      "Epoch [6/15], Step [54468/60000 (90.78%)], Loss: 0.1479\n",
      "Epoch [6/15], Step [57668/60000 (96.11%)], Loss: 0.1036\n",
      "Epoch: 6 Accuracy : 98.05 %\n",
      "Epoch [7/15], Step [3268/60000 (5.45%)], Loss: 0.0056\n",
      "Epoch [7/15], Step [6468/60000 (10.78%)], Loss: 0.0696\n",
      "Epoch [7/15], Step [9668/60000 (16.11%)], Loss: 0.0043\n",
      "Epoch [7/15], Step [12868/60000 (21.45%)], Loss: 0.1088\n",
      "Epoch [7/15], Step [16068/60000 (26.78%)], Loss: 0.2246\n",
      "Epoch [7/15], Step [19268/60000 (32.11%)], Loss: 0.1163\n",
      "Epoch [7/15], Step [22468/60000 (37.45%)], Loss: 0.2371\n",
      "Epoch [7/15], Step [25668/60000 (42.78%)], Loss: 0.0049\n",
      "Epoch [7/15], Step [28868/60000 (48.11%)], Loss: 0.0052\n",
      "Epoch [7/15], Step [32068/60000 (53.45%)], Loss: 0.3768\n",
      "Epoch [7/15], Step [35268/60000 (58.78%)], Loss: 0.1188\n",
      "Epoch [7/15], Step [38468/60000 (64.11%)], Loss: 0.0062\n",
      "Epoch [7/15], Step [41668/60000 (69.45%)], Loss: 0.0023\n",
      "Epoch [7/15], Step [44868/60000 (74.78%)], Loss: 0.0002\n",
      "Epoch [7/15], Step [48068/60000 (80.11%)], Loss: 0.2516\n",
      "Epoch [7/15], Step [51268/60000 (85.45%)], Loss: 0.0279\n",
      "Epoch [7/15], Step [54468/60000 (90.78%)], Loss: 0.0634\n",
      "Epoch [7/15], Step [57668/60000 (96.11%)], Loss: 0.0369\n",
      "Epoch: 7 Accuracy : 97.4 %\n",
      "Epoch [8/15], Step [3268/60000 (5.45%)], Loss: 0.0897\n",
      "Epoch [8/15], Step [6468/60000 (10.78%)], Loss: 0.0042\n",
      "Epoch [8/15], Step [9668/60000 (16.11%)], Loss: 0.0679\n",
      "Epoch [8/15], Step [12868/60000 (21.45%)], Loss: 0.0015\n",
      "Epoch [8/15], Step [16068/60000 (26.78%)], Loss: 0.0600\n",
      "Epoch [8/15], Step [19268/60000 (32.11%)], Loss: 0.0357\n",
      "Epoch [8/15], Step [22468/60000 (37.45%)], Loss: 0.0895\n",
      "Epoch [8/15], Step [25668/60000 (42.78%)], Loss: 0.0490\n",
      "Epoch [8/15], Step [28868/60000 (48.11%)], Loss: 0.0111\n",
      "Epoch [8/15], Step [32068/60000 (53.45%)], Loss: 0.2513\n",
      "Epoch [8/15], Step [35268/60000 (58.78%)], Loss: 0.0283\n",
      "Epoch [8/15], Step [38468/60000 (64.11%)], Loss: 0.2868\n",
      "Epoch [8/15], Step [41668/60000 (69.45%)], Loss: 0.2364\n",
      "Epoch [8/15], Step [44868/60000 (74.78%)], Loss: 0.1468\n",
      "Epoch [8/15], Step [48068/60000 (80.11%)], Loss: 0.0074\n",
      "Epoch [8/15], Step [51268/60000 (85.45%)], Loss: 0.0515\n",
      "Epoch [8/15], Step [54468/60000 (90.78%)], Loss: 0.1676\n",
      "Epoch [8/15], Step [57668/60000 (96.11%)], Loss: 0.0086\n",
      "Epoch: 8 Accuracy : 97.8 %\n",
      "Epoch [9/15], Step [3268/60000 (5.45%)], Loss: 0.3553\n",
      "Epoch [9/15], Step [6468/60000 (10.78%)], Loss: 0.0616\n",
      "Epoch [9/15], Step [9668/60000 (16.11%)], Loss: 0.0989\n",
      "Epoch [9/15], Step [12868/60000 (21.45%)], Loss: 0.0007\n",
      "Epoch [9/15], Step [16068/60000 (26.78%)], Loss: 0.1378\n",
      "Epoch [9/15], Step [19268/60000 (32.11%)], Loss: 0.1604\n",
      "Epoch [9/15], Step [22468/60000 (37.45%)], Loss: 0.0198\n",
      "Epoch [9/15], Step [25668/60000 (42.78%)], Loss: 0.0006\n",
      "Epoch [9/15], Step [28868/60000 (48.11%)], Loss: 0.0038\n",
      "Epoch [9/15], Step [32068/60000 (53.45%)], Loss: 0.0847\n",
      "Epoch [9/15], Step [35268/60000 (58.78%)], Loss: 0.0018\n",
      "Epoch [9/15], Step [38468/60000 (64.11%)], Loss: 0.0015\n",
      "Epoch [9/15], Step [41668/60000 (69.45%)], Loss: 0.0303\n",
      "Epoch [9/15], Step [44868/60000 (74.78%)], Loss: 0.0002\n",
      "Epoch [9/15], Step [48068/60000 (80.11%)], Loss: 0.0740\n",
      "Epoch [9/15], Step [51268/60000 (85.45%)], Loss: 0.1607\n",
      "Epoch [9/15], Step [54468/60000 (90.78%)], Loss: 0.0815\n",
      "Epoch [9/15], Step [57668/60000 (96.11%)], Loss: 0.0029\n",
      "Epoch: 9 Accuracy : 97.16 %\n",
      "Epoch [10/15], Step [3268/60000 (5.45%)], Loss: 0.0298\n",
      "Epoch [10/15], Step [6468/60000 (10.78%)], Loss: 0.0112\n",
      "Epoch [10/15], Step [9668/60000 (16.11%)], Loss: 0.0808\n",
      "Epoch [10/15], Step [12868/60000 (21.45%)], Loss: 0.0125\n",
      "Epoch [10/15], Step [16068/60000 (26.78%)], Loss: 0.1016\n",
      "Epoch [10/15], Step [19268/60000 (32.11%)], Loss: 0.3058\n",
      "Epoch [10/15], Step [22468/60000 (37.45%)], Loss: 0.0045\n",
      "Epoch [10/15], Step [25668/60000 (42.78%)], Loss: 0.0871\n",
      "Epoch [10/15], Step [28868/60000 (48.11%)], Loss: 0.0292\n",
      "Epoch [10/15], Step [32068/60000 (53.45%)], Loss: 0.0457\n",
      "Epoch [10/15], Step [35268/60000 (58.78%)], Loss: 0.0199\n",
      "Epoch [10/15], Step [38468/60000 (64.11%)], Loss: 0.0388\n",
      "Epoch [10/15], Step [41668/60000 (69.45%)], Loss: 0.4745\n",
      "Epoch [10/15], Step [44868/60000 (74.78%)], Loss: 0.1154\n",
      "Epoch [10/15], Step [48068/60000 (80.11%)], Loss: 0.2557\n",
      "Epoch [10/15], Step [51268/60000 (85.45%)], Loss: 0.0105\n",
      "Epoch [10/15], Step [54468/60000 (90.78%)], Loss: 0.0929\n",
      "Epoch [10/15], Step [57668/60000 (96.11%)], Loss: 0.2940\n",
      "Epoch: 10 Accuracy : 97.86 %\n",
      "Epoch [11/15], Step [3268/60000 (5.45%)], Loss: 0.0467\n",
      "Epoch [11/15], Step [6468/60000 (10.78%)], Loss: 0.0949\n",
      "Epoch [11/15], Step [9668/60000 (16.11%)], Loss: 0.0055\n",
      "Epoch [11/15], Step [12868/60000 (21.45%)], Loss: 0.0052\n",
      "Epoch [11/15], Step [16068/60000 (26.78%)], Loss: 0.0229\n",
      "Epoch [11/15], Step [19268/60000 (32.11%)], Loss: 0.0530\n",
      "Epoch [11/15], Step [22468/60000 (37.45%)], Loss: 0.0002\n",
      "Epoch [11/15], Step [25668/60000 (42.78%)], Loss: 0.1051\n",
      "Epoch [11/15], Step [28868/60000 (48.11%)], Loss: 0.0937\n",
      "Epoch [11/15], Step [32068/60000 (53.45%)], Loss: 0.0080\n",
      "Epoch [11/15], Step [35268/60000 (58.78%)], Loss: 0.0115\n",
      "Epoch [11/15], Step [38468/60000 (64.11%)], Loss: 0.0003\n",
      "Epoch [11/15], Step [41668/60000 (69.45%)], Loss: 0.2806\n",
      "Epoch [11/15], Step [44868/60000 (74.78%)], Loss: 0.2315\n",
      "Epoch [11/15], Step [48068/60000 (80.11%)], Loss: 0.0025\n",
      "Epoch [11/15], Step [51268/60000 (85.45%)], Loss: 0.0008\n",
      "Epoch [11/15], Step [54468/60000 (90.78%)], Loss: 0.1096\n",
      "Epoch [11/15], Step [57668/60000 (96.11%)], Loss: 0.0048\n",
      "Epoch: 11 Accuracy : 97.36 %\n",
      "Epoch [12/15], Step [3268/60000 (5.45%)], Loss: 0.0015\n",
      "Epoch [12/15], Step [6468/60000 (10.78%)], Loss: 0.1456\n",
      "Epoch [12/15], Step [9668/60000 (16.11%)], Loss: 0.0438\n",
      "Epoch [12/15], Step [12868/60000 (21.45%)], Loss: 0.0151\n",
      "Epoch [12/15], Step [16068/60000 (26.78%)], Loss: 0.1067\n",
      "Epoch [12/15], Step [19268/60000 (32.11%)], Loss: 0.0267\n",
      "Epoch [12/15], Step [22468/60000 (37.45%)], Loss: 0.0417\n",
      "Epoch [12/15], Step [25668/60000 (42.78%)], Loss: 0.1693\n",
      "Epoch [12/15], Step [28868/60000 (48.11%)], Loss: 0.0013\n",
      "Epoch [12/15], Step [32068/60000 (53.45%)], Loss: 0.0051\n",
      "Epoch [12/15], Step [35268/60000 (58.78%)], Loss: 0.0074\n",
      "Epoch [12/15], Step [38468/60000 (64.11%)], Loss: 0.1245\n",
      "Epoch [12/15], Step [41668/60000 (69.45%)], Loss: 0.0846\n",
      "Epoch [12/15], Step [44868/60000 (74.78%)], Loss: 0.0709\n",
      "Epoch [12/15], Step [48068/60000 (80.11%)], Loss: 0.0567\n",
      "Epoch [12/15], Step [51268/60000 (85.45%)], Loss: 0.0029\n",
      "Epoch [12/15], Step [54468/60000 (90.78%)], Loss: 0.0111\n",
      "Epoch [12/15], Step [57668/60000 (96.11%)], Loss: 0.1679\n",
      "Epoch: 12 Accuracy : 97.89 %\n",
      "Epoch [13/15], Step [3268/60000 (5.45%)], Loss: 0.2144\n",
      "Epoch [13/15], Step [6468/60000 (10.78%)], Loss: 0.0382\n",
      "Epoch [13/15], Step [9668/60000 (16.11%)], Loss: 0.2090\n",
      "Epoch [13/15], Step [12868/60000 (21.45%)], Loss: 0.0532\n",
      "Epoch [13/15], Step [16068/60000 (26.78%)], Loss: 0.0998\n",
      "Epoch [13/15], Step [19268/60000 (32.11%)], Loss: 0.0152\n",
      "Epoch [13/15], Step [22468/60000 (37.45%)], Loss: 0.1657\n",
      "Epoch [13/15], Step [25668/60000 (42.78%)], Loss: 0.4551\n",
      "Epoch [13/15], Step [28868/60000 (48.11%)], Loss: 0.0320\n",
      "Epoch [13/15], Step [32068/60000 (53.45%)], Loss: 0.1321\n",
      "Epoch [13/15], Step [35268/60000 (58.78%)], Loss: 0.1335\n",
      "Epoch [13/15], Step [38468/60000 (64.11%)], Loss: 0.0293\n",
      "Epoch [13/15], Step [41668/60000 (69.45%)], Loss: 0.0002\n",
      "Epoch [13/15], Step [44868/60000 (74.78%)], Loss: 0.0215\n",
      "Epoch [13/15], Step [48068/60000 (80.11%)], Loss: 0.1544\n",
      "Epoch [13/15], Step [51268/60000 (85.45%)], Loss: 0.0012\n",
      "Epoch [13/15], Step [54468/60000 (90.78%)], Loss: 0.0154\n",
      "Epoch [13/15], Step [57668/60000 (96.11%)], Loss: 0.0196\n",
      "Epoch: 13 Accuracy : 97.76 %\n",
      "Epoch [14/15], Step [3268/60000 (5.45%)], Loss: 0.1755\n",
      "Epoch [14/15], Step [6468/60000 (10.78%)], Loss: 0.0170\n",
      "Epoch [14/15], Step [9668/60000 (16.11%)], Loss: 0.0012\n",
      "Epoch [14/15], Step [12868/60000 (21.45%)], Loss: 0.0140\n",
      "Epoch [14/15], Step [16068/60000 (26.78%)], Loss: 0.0959\n",
      "Epoch [14/15], Step [19268/60000 (32.11%)], Loss: 0.0004\n",
      "Epoch [14/15], Step [22468/60000 (37.45%)], Loss: 0.0174\n",
      "Epoch [14/15], Step [25668/60000 (42.78%)], Loss: 0.0759\n",
      "Epoch [14/15], Step [28868/60000 (48.11%)], Loss: 0.0027\n",
      "Epoch [14/15], Step [32068/60000 (53.45%)], Loss: 0.0055\n",
      "Epoch [14/15], Step [35268/60000 (58.78%)], Loss: 0.4475\n",
      "Epoch [14/15], Step [38468/60000 (64.11%)], Loss: 0.0056\n",
      "Epoch [14/15], Step [41668/60000 (69.45%)], Loss: 0.1287\n",
      "Epoch [14/15], Step [44868/60000 (74.78%)], Loss: 0.0005\n",
      "Epoch [14/15], Step [48068/60000 (80.11%)], Loss: 0.1119\n",
      "Epoch [14/15], Step [51268/60000 (85.45%)], Loss: 0.0340\n",
      "Epoch [14/15], Step [54468/60000 (90.78%)], Loss: 0.0086\n",
      "Epoch [14/15], Step [57668/60000 (96.11%)], Loss: 0.0819\n",
      "Epoch: 14 Accuracy : 97.37 %\n",
      "Epoch [15/15], Step [3268/60000 (5.45%)], Loss: 0.1038\n",
      "Epoch [15/15], Step [6468/60000 (10.78%)], Loss: 0.0380\n",
      "Epoch [15/15], Step [9668/60000 (16.11%)], Loss: 0.0197\n",
      "Epoch [15/15], Step [12868/60000 (21.45%)], Loss: 0.0765\n",
      "Epoch [15/15], Step [16068/60000 (26.78%)], Loss: 0.1325\n",
      "Epoch [15/15], Step [19268/60000 (32.11%)], Loss: 0.0041\n",
      "Epoch [15/15], Step [22468/60000 (37.45%)], Loss: 0.0553\n",
      "Epoch [15/15], Step [25668/60000 (42.78%)], Loss: 0.0484\n",
      "Epoch [15/15], Step [28868/60000 (48.11%)], Loss: 0.1016\n",
      "Epoch [15/15], Step [32068/60000 (53.45%)], Loss: 0.0848\n",
      "Epoch [15/15], Step [35268/60000 (58.78%)], Loss: 0.0326\n",
      "Epoch [15/15], Step [38468/60000 (64.11%)], Loss: 0.0453\n",
      "Epoch [15/15], Step [41668/60000 (69.45%)], Loss: 0.0463\n",
      "Epoch [15/15], Step [44868/60000 (74.78%)], Loss: 0.0013\n",
      "Epoch [15/15], Step [48068/60000 (80.11%)], Loss: 0.1543\n",
      "Epoch [15/15], Step [51268/60000 (85.45%)], Loss: 0.0139\n",
      "Epoch [15/15], Step [54468/60000 (90.78%)], Loss: 0.1558\n",
      "Epoch [15/15], Step [57668/60000 (96.11%)], Loss: 0.0084\n",
      "Epoch: 15 Accuracy : 97.34 %\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(num_epochs, cnn, loaders):\n",
    "    \n",
    "    cnn.train()\n",
    "    accuracies = []\n",
    "    max_accuracy = 0\n",
    "    list_loses = []\n",
    "        \n",
    "    # Train the model\n",
    "    total_step = len(loaders)\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        correct = 0\n",
    "        for i, (images, labels) in enumerate(loaders):\n",
    "            \n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(images).float()\n",
    "            b_y = Variable(labels)   # batch y\n",
    "            output = cnn(b_x)[0]               \n",
    "            loss = loss_function_cnn(output, b_y)\n",
    "            \n",
    "            # clear gradients for this training step   \n",
    "            optimizer_cnn.zero_grad()           \n",
    "            \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()    \n",
    "            # apply gradients             \n",
    "            optimizer_cnn.step()  \n",
    "             \n",
    "            #store loss in list\n",
    "            list_loses.append(round(loss.item(),4))\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                            print ('Epoch [{}/{}], Step [{}/{} ({:.2f}%)], Loss: {:.4f}'\n",
    "                                   .format(epoch + 1, num_epochs, i*len(b_x)+100, len(loaders.dataset), (i*len(b_x)+100)*100/(len(loaders.dataset)), loss.item()))\n",
    "                \n",
    "            pass\n",
    "        accuracy = float(find_accuracy(cnn, test_loader))\n",
    "        accuracies.append(accuracy)\n",
    "        if accuracy > max_accuracy:\n",
    "            best_model = copy.deepcopy(cnn)\n",
    "            max_accuracy = accuracy\n",
    "        \n",
    "        print('Epoch:', epoch+1, \"Accuracy :\", accuracy, '%')\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    return best_model, accuracies, list_loses\n",
    "\n",
    "\n",
    "\n",
    "#train(15, cnn, train_loader)\n",
    "\n",
    "best_model, accuracies, list_loses = train(15, cnn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-breast",
   "metadata": {},
   "source": [
    "### 2.h Plot the results\n",
    "Again, you should plot the evolution of your error metrics (accuracy and loss function) in function of the number of epochs, to see how efficient the training is. Show the evolution of the images generated from your reference sample as well.\n",
    "\n",
    "Task: Make those plots, as clean and readable as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "special-testimony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Accuracy",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15
         ],
         "y": [
          96.77,
          97.31,
          97.78,
          97.23,
          97.44,
          98.05,
          97.4,
          97.8,
          97.16,
          97.86,
          97.36,
          97.89,
          97.76,
          97.37,
          97.34
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Accuracy over epochs"
        },
        "xaxis": {
         "title": {
          "text": "Epochs"
         }
        },
        "yaxis": {
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plots of accuracy and of loss function, images generated from your reference sample.\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#plot accuracies with plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=list(range(1,16)), y=accuracies, mode='lines+markers', name='Accuracy'))\n",
    "fig.update_layout(title='Accuracy over epochs', xaxis_title='Epochs', yaxis_title='Accuracy')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "#plot loss with plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "05dbfbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125.0"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_loses)/15**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-network",
   "metadata": {},
   "source": [
    "## Part 2 coming soon\n",
    "You now have operational GANs and CNNs for handwritten digits (and/or letters). In the second part of the homework, you will combine both networks in order to create what you want and learn more about those neural architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e593ac106456af50ce7af38f9671c411b49d6cd90f9b885e167f0f594e09038c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
